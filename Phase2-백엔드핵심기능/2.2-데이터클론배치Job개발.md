# 2.2 데이터 클론 배치 Job 개발

## 개요
실제 데이터베이스 간 데이터 복제를 수행하는 배치 Job의 핵심 구성요소들을 개발하는 단계입니다.

## 작업 체크리스트

### FAS 스키마 테이블 메타데이터 조회 기능

#### DatabaseMetadataService.java 생성
- [ ] src/main/java/com/sbs/datasync/service/DatabaseMetadataService.java 파일 생성
```java
package com.sbs.datasync.service;

import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.stereotype.Service;

import javax.sql.DataSource;
import java.sql.*;
import java.util.*;

@Service
public class DatabaseMetadataService {

    private final DataSource sourceDataSource;

    public DatabaseMetadataService(@Qualifier("sourceDataSource") DataSource sourceDataSource) {
        this.sourceDataSource = sourceDataSource;
    }

    /**
     * 스키마의 모든 테이블 목록 조회
     */
    public List<String> getTableNames(String schemaName) throws SQLException {
        List<String> tableNames = new ArrayList<>();

        try (Connection conn = sourceDataSource.getConnection()) {
            DatabaseMetaData metaData = conn.getMetaData();
            ResultSet tables = metaData.getTables(null, schemaName.toUpperCase(), "%", new String[]{"TABLE"});

            while (tables.next()) {
                String tableName = tables.getString("TABLE_NAME");
                tableNames.add(tableName);
            }
        }

        return tableNames;
    }

    /**
     * 테이블의 컬럼 정보 조회
     */
    public List<ColumnInfo> getColumnInfo(String schemaName, String tableName) throws SQLException {
        List<ColumnInfo> columns = new ArrayList<>();

        try (Connection conn = sourceDataSource.getConnection()) {
            DatabaseMetaData metaData = conn.getMetaData();
            ResultSet columnSet = metaData.getColumns(null, schemaName.toUpperCase(),
                                                     tableName.toUpperCase(), "%");

            while (columnSet.next()) {
                ColumnInfo column = new ColumnInfo(
                    columnSet.getString("COLUMN_NAME"),
                    columnSet.getInt("DATA_TYPE"),
                    columnSet.getString("TYPE_NAME"),
                    columnSet.getInt("COLUMN_SIZE"),
                    columnSet.getInt("NULLABLE") == DatabaseMetaData.columnNullable,
                    columnSet.getString("COLUMN_DEF")
                );
                columns.add(column);
            }
        }

        return columns;
    }

    /**
     * 테이블의 Primary Key 정보 조회
     */
    public List<String> getPrimaryKeys(String schemaName, String tableName) throws SQLException {
        List<String> primaryKeys = new ArrayList<>();

        try (Connection conn = sourceDataSource.getConnection()) {
            DatabaseMetaData metaData = conn.getMetaData();
            ResultSet pkSet = metaData.getPrimaryKeys(null, schemaName.toUpperCase(),
                                                     tableName.toUpperCase());

            while (pkSet.next()) {
                primaryKeys.add(pkSet.getString("COLUMN_NAME"));
            }
        }

        return primaryKeys;
    }

    /**
     * 테이블의 레코드 수 조회
     */
    public long getTableRowCount(String schemaName, String tableName) throws SQLException {
        String sql = String.format("SELECT COUNT(*) FROM %s.%s", schemaName, tableName);

        try (Connection conn = sourceDataSource.getConnection();
             PreparedStatement stmt = conn.prepareStatement(sql);
             ResultSet rs = stmt.executeQuery()) {

            if (rs.next()) {
                return rs.getLong(1);
            }
        }

        return 0;
    }

    /**
     * 조건에 따른 레코드 수 조회
     */
    public long getTableRowCount(String schemaName, String tableName, String whereClause) throws SQLException {
        String sql = String.format("SELECT COUNT(*) FROM %s.%s", schemaName, tableName);
        if (whereClause != null && !whereClause.trim().isEmpty()) {
            sql += " WHERE " + whereClause;
        }

        try (Connection conn = sourceDataSource.getConnection();
             PreparedStatement stmt = conn.prepareStatement(sql);
             ResultSet rs = stmt.executeQuery()) {

            if (rs.next()) {
                return rs.getLong(1);
            }
        }

        return 0;
    }

    /**
     * 테이블 존재 여부 확인
     */
    public boolean tableExists(String schemaName, String tableName) throws SQLException {
        try (Connection conn = sourceDataSource.getConnection()) {
            DatabaseMetaData metaData = conn.getMetaData();
            ResultSet tables = metaData.getTables(null, schemaName.toUpperCase(),
                                                 tableName.toUpperCase(), new String[]{"TABLE"});
            return tables.next();
        }
    }

    /**
     * 컬럼 정보 DTO
     */
    public static class ColumnInfo {
        private final String columnName;
        private final int dataType;
        private final String typeName;
        private final int columnSize;
        private final boolean nullable;
        private final String defaultValue;

        public ColumnInfo(String columnName, int dataType, String typeName,
                         int columnSize, boolean nullable, String defaultValue) {
            this.columnName = columnName;
            this.dataType = dataType;
            this.typeName = typeName;
            this.columnSize = columnSize;
            this.nullable = nullable;
            this.defaultValue = defaultValue;
        }

        // Getters
        public String getColumnName() { return columnName; }
        public int getDataType() { return dataType; }
        public String getTypeName() { return typeName; }
        public int getColumnSize() { return columnSize; }
        public boolean isNullable() { return nullable; }
        public String getDefaultValue() { return defaultValue; }
    }
}
```

### 동적 쿼리 생성 Reader 개발

#### DynamicJdbcItemReader.java 생성
- [ ] src/main/java/com/sbs/datasync/batch/reader/DynamicJdbcItemReader.java 파일 생성
```java
package com.sbs.datasync.batch.reader;

import com.sbs.datasync.entity.BatchJobConfig;
import com.sbs.datasync.service.DatabaseMetadataService;
import org.springframework.batch.item.database.JdbcCursorItemReader;
import org.springframework.batch.item.database.builder.JdbcCursorItemReaderBuilder;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.RowMapper;
import org.springframework.stereotype.Component;

import javax.sql.DataSource;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.time.LocalDate;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

@Component
public class DynamicJdbcItemReader {

    private final DataSource sourceDataSource;
    private final DatabaseMetadataService metadataService;

    public DynamicJdbcItemReader(@Qualifier("sourceDataSource") DataSource sourceDataSource,
                                DatabaseMetadataService metadataService) {
        this.sourceDataSource = sourceDataSource;
        this.metadataService = metadataService;
    }

    public JdbcCursorItemReader<Map<String, Object>> createReader(BatchJobConfig jobConfig) throws SQLException {
        String sql = buildSelectQuery(jobConfig);

        return new JdbcCursorItemReaderBuilder<Map<String, Object>>()
                .name("dynamicJdbcReader")
                .dataSource(sourceDataSource)
                .sql(sql)
                .rowMapper(new DynamicRowMapper(jobConfig.getSchemaName(), jobConfig.getTableName()))
                .fetchSize(1000)  // 성능 최적화를 위한 fetch size 설정
                .maxItemCount(Integer.MAX_VALUE)
                .build();
    }

    private String buildSelectQuery(BatchJobConfig jobConfig) throws SQLException {
        String schemaName = jobConfig.getSchemaName();
        String tableName = jobConfig.getTableName();

        // 사용자 정의 쿼리가 있으면 그것을 사용
        if (jobConfig.getInsertQuery() != null && !jobConfig.getInsertQuery().trim().isEmpty()) {
            return jobConfig.getInsertQuery();
        }

        // 기본 SELECT 쿼리 생성
        List<DatabaseMetadataService.ColumnInfo> columns = metadataService.getColumnInfo(schemaName, tableName);
        StringBuilder selectClause = new StringBuilder("SELECT ");

        for (int i = 0; i < columns.size(); i++) {
            if (i > 0) selectClause.append(", ");
            selectClause.append(columns.get(i).getColumnName());
        }

        selectClause.append(" FROM ").append(schemaName).append(".").append(tableName);

        // 날짜 조건 추가
        String whereClause = buildWhereClause(jobConfig, columns);
        if (!whereClause.isEmpty()) {
            selectClause.append(" WHERE ").append(whereClause);
        }

        return selectClause.toString();
    }

    private String buildWhereClause(BatchJobConfig jobConfig, List<DatabaseMetadataService.ColumnInfo> columns) {
        StringBuilder whereClause = new StringBuilder();

        // 날짜 범위 조건 구성
        LocalDate startDate = jobConfig.getStartDate();
        LocalDate endDate = jobConfig.getEndDate();

        if (startDate != null || endDate != null) {
            // 테이블에서 날짜 컬럼 찾기 (일반적인 날짜 컬럼명들)
            String[] dateColumns = {"CREATE_DATE", "CREATED_DATE", "REG_DATE", "UPDATED_DATE", "MODIFY_DATE"};
            String targetDateColumn = null;

            for (String dateCol : dateColumns) {
                for (DatabaseMetadataService.ColumnInfo column : columns) {
                    if (column.getColumnName().equals(dateCol)) {
                        targetDateColumn = dateCol;
                        break;
                    }
                }
                if (targetDateColumn != null) break;
            }

            if (targetDateColumn != null) {
                if (startDate != null && endDate != null) {
                    whereClause.append(targetDateColumn)
                              .append(" BETWEEN '").append(startDate).append("' AND '").append(endDate).append("'");
                } else if (startDate != null) {
                    whereClause.append(targetDateColumn).append(" >= '").append(startDate).append("'");
                } else {
                    whereClause.append(targetDateColumn).append(" <= '").append(endDate).append("'");
                }
            }
        }

        return whereClause.toString();
    }

    /**
     * 동적 RowMapper 구현
     */
    private class DynamicRowMapper implements RowMapper<Map<String, Object>> {
        private final String schemaName;
        private final String tableName;
        private List<DatabaseMetadataService.ColumnInfo> columnInfos;

        public DynamicRowMapper(String schemaName, String tableName) {
            this.schemaName = schemaName;
            this.tableName = tableName;
            try {
                this.columnInfos = metadataService.getColumnInfo(schemaName, tableName);
            } catch (SQLException e) {
                throw new RuntimeException("컬럼 정보 조회 실패", e);
            }
        }

        @Override
        public Map<String, Object> mapRow(ResultSet rs, int rowNum) throws SQLException {
            Map<String, Object> row = new HashMap<>();

            for (DatabaseMetadataService.ColumnInfo column : columnInfos) {
                String columnName = column.getColumnName();
                Object value = rs.getObject(columnName);
                row.put(columnName, value);
            }

            return row;
        }
    }
}
```

### 데이터 변환 Processor 개발

#### DataTransformationProcessor.java 생성
- [ ] src/main/java/com/sbs/datasync/batch/processor/DataTransformationProcessor.java 파일 생성
```java
package com.sbs.datasync.batch.processor;

import org.springframework.batch.item.ItemProcessor;
import org.springframework.stereotype.Component;

import java.time.LocalDateTime;
import java.util.Map;

@Component
public class DataTransformationProcessor implements ItemProcessor<Map<String, Object>, Map<String, Object>> {

    @Override
    public Map<String, Object> process(Map<String, Object> item) throws Exception {
        // 데이터 변환 로직
        Map<String, Object> transformedItem = processItem(item);

        // 검증
        if (!validateItem(transformedItem)) {
            return null; // null 반환 시 해당 아이템은 Writer로 전달되지 않음
        }

        return transformedItem;
    }

    private Map<String, Object> processItem(Map<String, Object> item) {
        // 1. 널 값 처리
        handleNullValues(item);

        // 2. 데이터 타입 변환
        convertDataTypes(item);

        // 3. 필드명 매핑 (필요시)
        mapFieldNames(item);

        // 4. 추가 메타데이터 설정
        addMetadata(item);

        return item;
    }

    private void handleNullValues(Map<String, Object> item) {
        // 특정 컬럼의 null 값을 기본값으로 변환
        item.forEach((key, value) -> {
            if (value == null) {
                switch (key.toUpperCase()) {
                    case "STATUS":
                        item.put(key, "ACTIVE");
                        break;
                    case "CREATED_DATE":
                    case "UPDATED_DATE":
                        item.put(key, LocalDateTime.now());
                        break;
                    default:
                        // 기본적으로 null 유지
                        break;
                }
            }
        });
    }

    private void convertDataTypes(Map<String, Object> item) {
        // 데이터 타입 변환 로직
        item.forEach((key, value) -> {
            if (value != null) {
                // String으로 된 날짜를 LocalDateTime으로 변환 등
                // 필요에 따라 구현
            }
        });
    }

    private void mapFieldNames(Map<String, Object> item) {
        // 원본 시스템과 대상 시스템 간의 필드명 차이 처리
        // 예: OLD_FIELD_NAME -> NEW_FIELD_NAME

        // 예시: 회계 시스템 특화 필드 매핑
        if (item.containsKey("ACCT_CODE")) {
            item.put("ACCOUNT_CODE", item.get("ACCT_CODE"));
            // 원본 필드는 유지하거나 제거 선택 가능
        }
    }

    private void addMetadata(Map<String, Object> item) {
        // 배치 실행 정보 추가
        item.put("BATCH_PROCESSED_DATE", LocalDateTime.now());
        item.put("BATCH_PROCESS_TYPE", "DATA_SYNC");
    }

    private boolean validateItem(Map<String, Object> item) {
        // 필수 필드 검증
        String[] requiredFields = {"ID", "CREATED_DATE"}; // FAS 스키마에 맞게 조정

        for (String field : requiredFields) {
            if (!item.containsKey(field) || item.get(field) == null) {
                System.err.println("필수 필드 누락: " + field + " in item: " + item);
                return false;
            }
        }

        // 비즈니스 규칙 검증
        return validateBusinessRules(item);
    }

    private boolean validateBusinessRules(Map<String, Object> item) {
        // FAS 스키마 특화 비즈니스 규칙 검증

        // 예시: 회계 데이터 검증
        if (item.containsKey("AMOUNT") && item.get("AMOUNT") != null) {
            try {
                Double amount = Double.valueOf(item.get("AMOUNT").toString());
                if (amount < 0) {
                    System.err.println("음수 금액 발견: " + amount + " in item: " + item);
                    return false;
                }
            } catch (NumberFormatException e) {
                System.err.println("잘못된 금액 형식: " + item.get("AMOUNT"));
                return false;
            }
        }

        return true;
    }
}
```

### 타겟 DB Writer 개발

#### DynamicJdbcItemWriter.java 생성
- [ ] src/main/java/com/sbs/datasync/batch/writer/DynamicJdbcItemWriter.java 파일 생성
```java
package com.sbs.datasync.batch.writer;

import com.sbs.datasync.entity.BatchJobConfig;
import com.sbs.datasync.service.DatabaseMetadataService;
import org.springframework.batch.item.database.JdbcBatchItemWriter;
import org.springframework.batch.item.database.builder.JdbcBatchItemWriterBuilder;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.namedparam.MapSqlParameterSource;
import org.springframework.jdbc.core.namedparam.SqlParameterSource;
import org.springframework.stereotype.Component;

import javax.sql.DataSource;
import java.sql.SQLException;
import java.util.List;
import java.util.Map;

@Component
public class DynamicJdbcItemWriter {

    private final DataSource targetDataSource;
    private final DatabaseMetadataService metadataService;

    public DynamicJdbcItemWriter(@Qualifier("primaryDataSource") DataSource targetDataSource,
                                DatabaseMetadataService metadataService) {
        this.targetDataSource = targetDataSource;
        this.metadataService = metadataService;
    }

    public JdbcBatchItemWriter<Map<String, Object>> createWriter(BatchJobConfig jobConfig) throws SQLException {
        String insertSql = buildInsertQuery(jobConfig);

        return new JdbcBatchItemWriterBuilder<Map<String, Object>>()
                .dataSource(targetDataSource)
                .sql(insertSql)
                .itemSqlParameterSourceProvider(this::createParameterSource)
                .assertUpdates(true) // 업데이트된 행 수 검증
                .build();
    }

    private String buildInsertQuery(BatchJobConfig jobConfig) throws SQLException {
        String schemaName = jobConfig.getSchemaName();
        String tableName = jobConfig.getTableName();

        // 테이블 컬럼 정보 조회
        List<DatabaseMetadataService.ColumnInfo> columns = metadataService.getColumnInfo(schemaName, tableName);

        StringBuilder insertSql = new StringBuilder();
        StringBuilder valuesSql = new StringBuilder();

        insertSql.append("INSERT INTO ").append(schemaName).append(".").append(tableName).append(" (");
        valuesSql.append("VALUES (");

        for (int i = 0; i < columns.size(); i++) {
            String columnName = columns.get(i).getColumnName();

            if (i > 0) {
                insertSql.append(", ");
                valuesSql.append(", ");
            }

            insertSql.append(columnName);
            valuesSql.append(":").append(columnName);
        }

        insertSql.append(") ");
        valuesSql.append(")");

        return insertSql.toString() + valuesSql.toString();
    }

    private SqlParameterSource createParameterSource(Map<String, Object> item) {
        MapSqlParameterSource parameterSource = new MapSqlParameterSource();

        item.forEach(parameterSource::addValue);

        return parameterSource;
    }
}
```

### 배치 실행 상태 업데이트 Listener 개발

#### BatchExecutionListener.java 생성
- [ ] src/main/java/com/sbs/datasync/batch/listener/BatchExecutionListener.java 파일 생성
```java
package com.sbs.datasync.batch.listener;

import com.sbs.datasync.entity.BatchExecutionHistory;
import com.sbs.datasync.entity.BatchJobConfig;
import com.sbs.datasync.repository.BatchExecutionHistoryRepository;
import com.sbs.datasync.repository.BatchJobConfigRepository;
import org.springframework.batch.core.*;
import org.springframework.batch.core.listener.JobExecutionListenerSupport;
import org.springframework.batch.item.Chunk;
import org.springframework.batch.item.ItemWriteListener;
import org.springframework.stereotype.Component;

import java.time.LocalDateTime;
import java.util.List;
import java.util.Map;
import java.util.Optional;

@Component
public class BatchExecutionListener extends JobExecutionListenerSupport implements ItemWriteListener<Map<String, Object>> {

    private final BatchExecutionHistoryRepository historyRepository;
    private final BatchJobConfigRepository jobConfigRepository;

    private BatchExecutionHistory currentExecution;

    public BatchExecutionListener(BatchExecutionHistoryRepository historyRepository,
                                 BatchJobConfigRepository jobConfigRepository) {
        this.historyRepository = historyRepository;
        this.jobConfigRepository = jobConfigRepository;
    }

    @Override
    public void beforeJob(JobExecution jobExecution) {
        Long jobConfigId = jobExecution.getJobParameters().getLong("jobConfigId");
        String executedBy = jobExecution.getJobParameters().getString("executedBy");

        if (jobConfigId != null) {
            Optional<BatchJobConfig> jobConfigOpt = jobConfigRepository.findById(jobConfigId);

            if (jobConfigOpt.isPresent()) {
                BatchJobConfig jobConfig = jobConfigOpt.get();

                // 실행 히스토리 생성
                currentExecution = new BatchExecutionHistory();
                currentExecution.setJobConfig(jobConfig);
                currentExecution.setJobInstanceId(jobExecution.getJobInstance().getId());
                currentExecution.setJobExecutionId(jobExecution.getId());
                currentExecution.setExecutionStatus(BatchExecutionHistory.ExecutionStatus.STARTED);
                currentExecution.setStartTime(LocalDateTime.now());
                currentExecution.setExecutedBy(executedBy != null ? executedBy : "system");
                currentExecution.setExecutionParams(jobExecution.getJobParameters().toString());

                currentExecution = historyRepository.save(currentExecution);

                System.out.println("배치 실행 시작: " + jobConfig.getJobName());
            }
        }
    }

    @Override
    public void afterJob(JobExecution jobExecution) {
        if (currentExecution != null) {
            currentExecution.setEndTime(LocalDateTime.now());

            // Job 실행 결과에 따른 상태 업데이트
            if (jobExecution.getStatus() == BatchStatus.COMPLETED) {
                currentExecution.setExecutionStatus(BatchExecutionHistory.ExecutionStatus.COMPLETED);
            } else if (jobExecution.getStatus() == BatchStatus.FAILED) {
                currentExecution.setExecutionStatus(BatchExecutionHistory.ExecutionStatus.FAILED);

                // 에러 메시지 수집
                StringBuilder errorMessages = new StringBuilder();
                for (StepExecution stepExecution : jobExecution.getStepExecutions()) {
                    List<Throwable> failureExceptions = stepExecution.getFailureExceptions();
                    for (Throwable throwable : failureExceptions) {
                        errorMessages.append(throwable.getMessage()).append("\n");
                    }
                }
                currentExecution.setErrorMessage(errorMessages.toString());
            } else {
                currentExecution.setExecutionStatus(BatchExecutionHistory.ExecutionStatus.STOPPED);
            }

            // Step 실행 통계 수집
            collectStepStatistics(jobExecution);

            historyRepository.save(currentExecution);

            System.out.println("배치 실행 완료: " + currentExecution.getExecutionStatus());
            System.out.println("처리 결과 - 성공: " + currentExecution.getSuccessCount() +
                             ", 오류: " + currentExecution.getErrorCount());
        }
    }

    private void collectStepStatistics(JobExecution jobExecution) {
        long totalReadCount = 0;
        long totalWriteCount = 0;
        long totalSkipCount = 0;

        for (StepExecution stepExecution : jobExecution.getStepExecutions()) {
            totalReadCount += stepExecution.getReadCount();
            totalWriteCount += stepExecution.getWriteCount();
            totalSkipCount += stepExecution.getReadSkipCount() +
                            stepExecution.getWriteSkipCount() +
                            stepExecution.getProcessSkipCount();
        }

        currentExecution.setTotalCount(totalReadCount);
        currentExecution.setSuccessCount(totalWriteCount);
        currentExecution.setSkipCount(totalSkipCount);
        currentExecution.setErrorCount(totalReadCount - totalWriteCount - totalSkipCount);
    }

    // ItemWriteListener 구현
    @Override
    public void beforeWrite(Chunk<? extends Map<String, Object>> items) {
        // Write 전 처리 (필요시)
    }

    @Override
    public void afterWrite(Chunk<? extends Map<String, Object>> items) {
        // Write 후 처리
        if (currentExecution != null && items != null) {
            long currentSuccessCount = currentExecution.getSuccessCount() != null ?
                currentExecution.getSuccessCount() : 0L;
            currentExecution.setSuccessCount(currentSuccessCount + items.size());
        }
    }

    @Override
    public void onWriteError(Exception exception, Chunk<? extends Map<String, Object>> items) {
        // Write 에러 처리
        if (currentExecution != null) {
            long currentErrorCount = currentExecution.getErrorCount() != null ?
                currentExecution.getErrorCount() : 0L;
            currentExecution.setErrorCount(currentErrorCount + (items != null ? items.size() : 0));
        }

        System.err.println("Write 에러 발생: " + exception.getMessage());
    }
}
```

#### StepProgressListener.java 생성
- [ ] src/main/java/com/sbs/datasync/batch/listener/StepProgressListener.java 파일 생성
```java
package com.sbs.datasync.batch.listener;

import org.springframework.batch.core.ChunkListener;
import org.springframework.batch.core.StepExecution;
import org.springframework.batch.core.StepExecutionListener;
import org.springframework.batch.core.scope.context.ChunkContext;
import org.springframework.stereotype.Component;

@Component
public class StepProgressListener implements StepExecutionListener, ChunkListener {

    @Override
    public void beforeStep(StepExecution stepExecution) {
        System.out.println("=== Step 시작: " + stepExecution.getStepName() + " ===");
        System.out.println("예상 처리 건수: " + stepExecution.getJobExecution().getJobParameters().getString("expectedCount"));
    }

    @Override
    public ExitStatus afterStep(StepExecution stepExecution) {
        System.out.println("=== Step 완료: " + stepExecution.getStepName() + " ===");
        System.out.println("읽기 건수: " + stepExecution.getReadCount());
        System.out.println("처리 건수: " + stepExecution.getWriteCount());
        System.out.println("건너뛴 건수: " + stepExecution.getSkipCount());
        System.out.println("Step 상태: " + stepExecution.getStatus());

        if (stepExecution.getStatus().isUnsuccessful()) {
            return ExitStatus.FAILED;
        }

        return ExitStatus.COMPLETED;
    }

    @Override
    public void beforeChunk(ChunkContext context) {
        // 진행률 출력 (1000건마다)
        StepExecution stepExecution = context.getStepContext().getStepExecution();
        if (stepExecution.getReadCount() % 1000 == 0 && stepExecution.getReadCount() > 0) {
            System.out.println("진행 상황: " + stepExecution.getReadCount() + "건 처리 완료");
        }
    }

    @Override
    public void afterChunk(ChunkContext context) {
        // Chunk 처리 후 추가 작업 (필요시)
    }

    @Override
    public void afterChunkError(ChunkContext context) {
        System.err.println("Chunk 처리 중 오류 발생: " + context.getStepContext().getStepName());
    }
}
```

### 통합 Job 구성 업데이트

#### DataSyncJob.java 업데이트
- [ ] DataSyncJob.java를 실제 구성요소들과 연결하도록 업데이트
```java
@Configuration
public class DataSyncJob extends AbstractBatchJob {

    private final DynamicJdbcItemReader itemReader;
    private final DataTransformationProcessor processor;
    private final DynamicJdbcItemWriter itemWriter;
    private final BatchExecutionListener executionListener;
    private final StepProgressListener progressListener;

    public DataSyncJob(JobRepository jobRepository,
                       PlatformTransactionManager transactionManager,
                       DynamicJdbcItemReader itemReader,
                       DataTransformationProcessor processor,
                       DynamicJdbcItemWriter itemWriter,
                       BatchExecutionListener executionListener,
                       StepProgressListener progressListener) {
        super(jobRepository, transactionManager);
        this.itemReader = itemReader;
        this.processor = processor;
        this.itemWriter = itemWriter;
        this.executionListener = executionListener;
        this.progressListener = progressListener;
    }

    @Bean
    @Override
    public Job createJob() {
        return getJobBuilder()
                .listener(executionListener)
                .start(createMainStep())
                .build();
    }

    @Bean
    @Override
    protected Step createMainStep() {
        return getStepBuilder("dataSyncStep")
                .<Map<String, Object>, Map<String, Object>>chunk(100)
                .reader(createDynamicReader())
                .processor(processor)
                .writer(createDynamicWriter())
                .listener(progressListener)
                .build();
    }

    private JdbcCursorItemReader<Map<String, Object>> createDynamicReader() {
        // 실행 시점에 JobParameters에서 BatchJobConfig 정보를 받아서 Reader 생성
        // 이 부분은 JobScope Bean으로 분리하여 런타임에 생성하도록 구현
        return null; // 실제로는 @Bean @JobScope로 분리
    }

    private JdbcBatchItemWriter<Map<String, Object>> createDynamicWriter() {
        // 실행 시점에 JobParameters에서 BatchJobConfig 정보를 받아서 Writer 생성
        // 이 부분은 JobScope Bean으로 분리하여 런타임에 생성하도록 구현
        return null; // 실제로는 @Bean @JobScope로 분리
    }
}
```

## 완료 확인 방법

### 메타데이터 조회 테스트
- [ ] DatabaseMetadataService로 FAS 스키마 테이블 목록 조회 테스트
- [ ] 테이블 컬럼 정보 및 Primary Key 정보 조회 테스트

### Reader/Writer 동작 테스트
- [ ] DynamicJdbcItemReader로 샘플 데이터 읽기 테스트
- [ ] DynamicJdbcItemWriter로 타겟 DB에 데이터 쓰기 테스트

### Processor 변환 테스트
- [ ] DataTransformationProcessor의 데이터 변환 로직 테스트
- [ ] 유효성 검증 로직 테스트

### 전체 Job 실행 테스트
- [ ] 실제 BatchJobConfig를 사용한 end-to-end 테스트
- [ ] Listener를 통한 실행 히스토리 저장 확인

### 성능 테스트
- [ ] 대용량 데이터 처리 성능 측정
- [ ] 메모리 사용량 모니터링

## 주의 사항
- DB2 JDBC 드라이버 특성 고려
- 대용량 데이터 처리 시 메모리 관리
- 트랜잭션 경계 설정 주의
- 동시 실행 시 데이터 일관성 고려

## 다음 단계
2.3 REST API 개발로 진행